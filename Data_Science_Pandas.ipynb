{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *pandas* package is the most important tool at the disposal of Data Scientists and Analysts working in Python today. The powerful machine learning and glamorous visualization tools may get all the attention, but pandas is the backbone of most data projects. \n",
    "\n",
    ">\\[*pandas*\\] is derived from the term \"**pan**el **da**ta\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. — [Wikipedia](https://en.wikipedia.org/wiki/Pandas_%28software%29)\n",
    "\n",
    "If you're thinking about data science as a career, then it is imperative that one of the first things you do is learn pandas. In this post, we will go over the essential bits of information about pandas, including how to install it, its uses, and how it works with other common Python data analysis packages such as **matplotlib** and **sci-kit learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Pandas for?\n",
    "\n",
    "Pandas has so many uses that it might make sense to list the things it can't do instead of what it can do. \n",
    "\n",
    "This tool is essentially your data’s home. Through pandas, you get acquainted with your data by cleaning, transforming, and analyzing it. \n",
    "\n",
    "For example, say you want to explore a dataset stored in a CSV on your computer. Pandas will extract the data from that CSV into a DataFrame — a table, basically — then let you do things like:\n",
    "\n",
    "- Calculate statistics and answer questions about the data, like\n",
    "\n",
    "\n",
    "    - What's the average, median, max, or min of each column? \n",
    "    - Does column A correlate with column B?\n",
    "    - What does the distribution of data in column C look like?\n",
    "\n",
    "\n",
    "- Clean the data by doing things like removing missing values and filtering rows or columns by some criteria\n",
    "\n",
    "\n",
    "- Join data tables using a common column to both files as the key to join the tables \n",
    "\n",
    "\n",
    "- Store the cleaned, transformed data back into a CSV, other file or database\n",
    "\n",
    "\n",
    "Before you jump into the modeling or the complex visualizations you need to have a good understanding of the nature of your dataset and pandas is the best avenue through which to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Jupyter Notebook and Example Data from GitHub\n",
    "\n",
    "In the first video, we collected data from the County Health Rankings website and saved the data file as a CSV data file on my desktop.  In the next video, we described getting this file and your data from GitHub and moving the CHR file from the desktop to the directory your obtained from Github.  Make sure your CHR data file is in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Data wrangling is the process of transforming and mapping data from a raw data form into another format with the intent of making it more appropriate and valuable for data analytics.\n",
    "\n",
    "1. Data Discovery\n",
    "    \n",
    "    This all-encompassing term describes how to understand your data. This is the first step to familiarize yourself with your data. You will use the Data Dictionary and different PANDAS commands for this step:\n",
    "    \n",
    "\n",
    "2. Structuring \n",
    "    \n",
    "    The next step is to organize the data. Raw data is typically unorganized and much of it may not be useful for the end product. This step is important for easier computation and analysis in the later steps.\n",
    "    \n",
    "\n",
    "3. Cleaning \n",
    "    \n",
    "    There are many different forms of cleaning data, for example one form of cleaning data is catching dates formatted in a different way and another form is removing outliers that will skew results and also formatting null values. This step is important in assuring the overall quality of the data.\n",
    "    \n",
    "\n",
    "4. Enriching \n",
    "    \n",
    "    At this step determine whether or not additional data would benefit the data set that could be easily added.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use PANDAS to complete these data wrangling steps.\n",
    "\n",
    "### PANDAS First Steps Import\n",
    "\n",
    " PANDAS is a module of code that can be used in python.  To use this module or any other module you have to import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# to allow local htm files to be open (data dictionary)\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "\n",
    "#set pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#set pandas to display 250 rows\n",
    "pd.set_option('display.max_rows', 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the basic components of pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core components of pandas: Series and DataFrames\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`. \n",
    "\n",
    "A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of Series. \n",
    "\n",
    "<img src=\"series-and-dataframe.png\" width=600px />\n",
    "\n",
    "DataFrames and Series are quite similar in that many operations that you can do with one you can do with the other, such as filling in null values and calculating the mean.\n",
    "\n",
    "You'll see how these components work when we start working with data below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read in data\n",
    "\n",
    "You can read data from many types of data files, Excel files, JSON, files, XML files, CSV files,etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from CSVs\n",
    "\n",
    "For simple CSV files, all you need is a single line to load in the data:\n",
    "1. You need the relative path to the file\n",
    "2. File name\n",
    "\n",
    "Using the instructions above, you should have all your files in the same directory.  Therefore, you can just use the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ColonCancer.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When you get a warning using Pandas read_csv\n",
    "\n",
    "It basically means you are loading in a CSV that has a column that consists of multiple data types. For example: `1,5,a,b,c,3,2,a` has a mix of strings and integers. \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery\n",
    "\n",
    "### Viewing your data\n",
    "\n",
    "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference. We accomplish this with `.head()`\n",
    "\n",
    "df.head() outputs the first five rows of your DataFrame by default, but we could also pass a number as well: df.head(20) would output the top twenty rows.\n",
    "\n",
    "To see the last five rows use .tail(). tail() also accepts a number to display the desired last rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to determine how many rows and columns make up the dataframe.  We do this by using the `.shape` method. Note that `.shape` has no parentheses and is returns (rows, columns). \n",
    "\n",
    "You'll be going to `.shape` a lot when cleaning and transforming data. For example, you might filter some rows based on some criteria and then want to know quickly how many rows were subsetted, added, or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "We need to determine how each column is coded.  \n",
    "\n",
    "- Do the columns contain continuous data or catagorical data?  \n",
    "- Which of these columns are important to our project question or hypothesis?\n",
    "- How do we need to process our data to be usable (if it is a catagorical column  of 1-6, 9. What do those numbers mean?\n",
    "\n",
    "You can open the Inpatient Data Dictionary.htm file in the same directory as this notebook or run the cell below to display a link to the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('Inpatient Data Dictionary.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring and Cleaning Data\n",
    "\n",
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataset should not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows. \n",
    "\n",
    "To demonstrate, I have duplicated a small portion of this dataset. To look for the presence of duplicate rows, we will use \n",
    "\n",
    "`df.duplicated()` returns whether or not the row is duplicated\n",
    "\n",
    "`df.duplicated().sum()` returns the number or rows duplicated\n",
    "\n",
    "`df[df.duplicated()]` return the duplicated rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.drop_duplicates()` will remove the duplicated rows \n",
    "\n",
    "**We need to verify that the rows we are removing are equal the duplicate rows**\n",
    " \n",
    "We have already run `.shape` and we get 34818 rows\n",
    "we first call ` tempdf= df.drop_duplicates()` the `tempdf.shape`\n",
    "\n",
    "We know there are 79 duplicated rows so 34818 - 79 = 34739\n",
    "\n",
    "Next we determine if there are any duplicated rows in tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf= df.drop_duplicates()\n",
    "tempdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we  have varified we have removed the duplicated rows, we can remove them from our original dataframe using\n",
    "\n",
    "```\n",
    "df= df.drop_duplicates()\n",
    "df.shape\n",
    "```\n",
    "then \n",
    "```\n",
    "df.duplicated().sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using. \n",
    "\n",
    "Notice in our movies dataset we have some obvious missing values in the `PAT_CTY_CODE` and `SEX_CODE` columns as well as others. We'll look at how to handle those in a bit.\n",
    "\n",
    "Seeing the datatype quickly is actually quite useful.  Calling `.info()` will quickly point out that your column you thought was all integers are actually string objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with missing values\n",
    "\n",
    "When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
    "\n",
    "There are two options in dealing with nulls: \n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as **imputation**\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that have missing data that are important for our project - Patient County Codes\n",
    "\n",
    "For our purposes, we are going to remove the rows that have a missing PAT_CTY_CODE.  Again we will test our process by using the tempdf variable.  Depending on your study you may need to delete missing data from other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = df.dropna(subset=['PAT_CTY_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['PAT_CTY_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anytime you drop rows or subset a dataframe\n",
    "\n",
    "You need to reindex the rows using:\n",
    "`df = df.reset_index(drop=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, Any column that you will need, delete the missing data rows.  For my study, I just want male and female, hispanic.\n",
    "\n",
    "First I will remove the SEX_CODE U the delete the missing data from SEX_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(df.index[df['SEX_CODE'] == 'U'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['SEX_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[df['ETHNICITY'] == '`'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ETHNICITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding Data\n",
    "\n",
    "Depending on your study you may need to delete missing data from other columns.  However, if you can recode the data that prevents losing data from other columns.  For example, we need to recode Discharge.  Look at the data dictionary that describes how this is coded.  \n",
    "\n",
    "Discharge is coded by year and quarter - for example:\n",
    "- 2020Q1\n",
    "\n",
    "for 2020 Quarter 1\n",
    "\n",
    "We will not need to separate the data by quarter so we will recode by year only.\n",
    "\n",
    "First we will insert a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting a column\n",
    "\n",
    "In PANDAS all rows and columns start at 0.  So the `loc=2` in the `.insert()` command will insert a new coloumn at position 2.\n",
    "\n",
    "\n",
    "<img src=\"dataframe_insert.jpg\">\n",
    "\n",
    "We want to insert a new column after DISCHARGE called YEAR.  Since DISCHARGE is column 1 we will insert YEAR as column 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=2, column='YEAR',value ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the year from DISCHARGE we will use string extract method that will usq Q to split the string into two parts the year number and the quarter number.  So 2020Q1 will be split into 2020 and 1.  The parameter ` expand = True` return that split for every row of the dataframe.\n",
    "\n",
    "` df['YEAR'] = df['DISCHARGE].str.split('Q',expand = True)[0]`\n",
    "\n",
    "`df['DISCHARGE].str.split('Q'expand = True)` returns a list of two values (year, quarter) for each row.  \n",
    "by adding [0] at the end of the command - ` df['YEAR','a'] = df['DISCHARGE].str.split('Q',expand = True)[0]` we take the first value of the list for each row (year)\n",
    "\n",
    "The full command ` df['YEAR'] = df['DISCHARGE].str.split('Q',expand = True)[0]` puts the first value in the column YEAR that we just inserted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YEAR'] = df['DISCHARGE'].str.split('Q', expand = True)[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding Data\n",
    "\n",
    "Depending on your study you may need to delete missing data from other columns.  However, if you can recode the data that prevents losing data from other columns.  For example, we should recode race and ethnicity.  Look at the data dictionary that describes how these are coded.  \n",
    "\n",
    "For Race:\n",
    "- 1 American Indian/Eskimo/Aleut\n",
    "- 2 Asian or Pacific Islander\n",
    "- 3 Black\n",
    "- 4 White\n",
    "- 5 Other\n",
    "- ` Invalid\n",
    "\n",
    "We should also look at how many rows are in each classification by using:\n",
    "\n",
    "```\n",
    "print(df['RACE'].value_counts())\n",
    "```\n",
    "\n",
    "If we have any Invalid classifications, we could recode those as Other.  In this case we do not.  We will use Pat_Age_Code to demonstrate recoding into a new inserted colon - RACE_NAME. RACE is column 14 so let's insert RACE_NAME as column 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=15, column='RACE_NAME',value ='')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['RACE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results above are numbers and not strings, we can recode as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['RACE'] == 1, 'RACE_NAME'] = 'American Indian/Eskimo/Aleut'\n",
    "df.loc[df['RACE'] == 2, 'RACE_NAME'] = 'Asian or Pacific Islander'\n",
    "df.loc[df['RACE'] == 3, 'RACE_NAME'] = 'Black'\n",
    "df.loc[df['RACE'] == 4, 'RACE_NAME'] = 'White'\n",
    "df.loc[df['RACE'] == 5, 'RACE_NAME'] = 'Other'\n",
    "\n",
    "df\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for ETHNICITY.  At this point ETNICITY is column 16 so insert ETHNICITY_NAME as column 17 and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=17, column='ETHNICITY_NAME',value ='')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['ETHNICITY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ETHNICITY'] == '1', 'ETHNICITY_NAME'] = 'Hispanic Origin'\n",
    "df.loc[df['ETHNICITY'] == '2', 'ETHNICITY_NAME'] = 'Not of Hispanic Origin'\n",
    "\n",
    "\n",
    "df\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data dictionary that describes how these are coded. ' ` '  is coded as invalid. In most cases this is missing or refused to answer.  For now we will recoded it as Missing.  You can decide if you need to delete these rows.\n",
    "\n",
    "Also since we have a mixed column of integers and the string (' ` '), we need to change the code to look for strings (== '1') and not integers (== 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['SEX_CODE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many rows are in each classification of PAT_AGE_CODE by using:\n",
    "\n",
    "```\n",
    "print(df['PAT_AGE_CODE'].value_counts())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['PAT_AGE_CODE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding PAT_AGE_CODE\n",
    "When we look at the PAT_AGE_CODE in the data dictionary, we see that there are two coding schemes depending on whether the patient has HIV/drug use or not.\n",
    "\n",
    "For our purposes we are going to recode to ages < 18 and 18 + . So we will recode as follows:\n",
    "\n",
    "| Original Codes | Classification |\n",
    "| ------ | ------ |\n",
    "| 0 - 5, 22 |< 18|\n",
    "| 6 - 21, 23-26 | 18 +|\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=20, column='PAT_AGE_GROUP',value ='')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['PAT_AGE_CODE'] <= 5, 'PAT_AGE_GROUP'] = '< 18'\n",
    "\n",
    "#now I set all other group including code 22 to >18\n",
    "df.loc[df['PAT_AGE_CODE'] > 5, 'PAT_AGE_GROUP'] = '> 18'\n",
    "\n",
    "# now set code 22 to < 18\n",
    "df.loc[df['PAT_AGE_CODE'] == 22, 'PAT_AGE_GROUP'] = '< 18'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other recoding\n",
    "\n",
    "Use the processes described above to recode any other columns you may want to use.  If you are having trouble, email me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplifying the dataset to just the columns you will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a column as a *DataFrame*, you need to pass a list of column names. In the next case that's just a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = df[['LENGTH_OF_STAY']]\n",
    "newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adding another column name is easy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = df[['LENGTH_OF_STAY', 'PAT_COUNTY']]\n",
    "\n",
    "newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude hospitalizations in children\n",
    "\n",
    "\n",
    "### Subsetting by Conditional selections\n",
    "\n",
    "\n",
    "In your data set we will need to subset the data depending on how you use the data.  \n",
    "\n",
    "Since I only need adults for my study, I will only keep the rows where the `PAT_AGE_GROUP == '> 18'`\n",
    "\n",
    "We will do this again later to get the proper YEAR of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['PAT_AGE_GROUP'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['PAT_AGE_GROUP'] == '> 18']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `describe()` on an entire DataFrame we can get a summary of the distribution of continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LENGTH_OF_STAY'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.describe()` can also be used on a categorical variable to get the count of rows, unique count of categories, top category, and freq of top category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PUBLIC_HEALTH_REGION'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the `PUBLIC_HEALTH_REGION` column has 11 unique values, the top value is 3.0, which shows up 8450 times (freq).\n",
    "\n",
    "`.value_counts()` can tell us the frequency of all values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PUBLIC_HEALTH_REGION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PAT_COUNTY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at the column names of our data set by using `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copied the column names I want to keep and pasted them into:\n",
    "\n",
    "```\n",
    "df[['RECORD_ID', 'YEAR', 'PAT_CTY_CODE', 'PAT_COUNTY', \n",
    "       'SEX_CODE',   'ETHNICITY_NAME']]\n",
    "```\n",
    "and set it equal to a new dataframe `df1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['RECORD_ID', 'YEAR', 'PAT_CTY_CODE', 'PAT_COUNTY', \n",
    "       'SEX_CODE',   'ETHNICITY_NAME']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to a CSV\n",
    "\n",
    "So after extensive work on cleaning your data, you’re now ready to save it as a file. Similar to the way we read in data, pandas provides intuitive commands to save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('myData.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you reach this point in processing your data\n",
    "\n",
    "You have saved your previous work as `myData.csv`\n",
    "\n",
    "**If you need to stop, this is a safe place to stop.**  \n",
    "\n",
    "When you come back to this notebook, run the cell that \n",
    "`imports pandas as pd....` then start with the next cell which will load myData.csv and assign categorical and continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we saved the dataset we saved the following columns and using the data dictionary we can determine which columns are categorical and which are continuous.\n",
    "\n",
    "| Column | Categorical or Continuous |\n",
    "|--------|---------|\n",
    "| RECORD_ID | - |\n",
    "|YEAR | CATEGORICAL |\n",
    "|PAT_COUNTY| CATEGORICAL |\n",
    "|SEX_CODE| CATEGORICAL |\n",
    "|ETHNICITY_NAME | CATEGORICAL |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('myData.csv', dtype={'RECORD_ID': 'category', 'YEAR': 'category', 'PAT_COUNTY': 'category',\n",
    "                                      'SEX_CODE': 'category',  'ETHNICITY_NAME': 'category'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Selecting, Extracting, Subsetting a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting by column\n",
    "\n",
    "You already saw how to extract a columns and make a new dataframe using the double brackets like this:\n",
    "\n",
    "```\n",
    "df1 = df[['RECORD_ID', 'YEAR', 'PAT_COUNTY', 'PUBLIC_HEALTH_REGION', \n",
    "       'SEX_CODE',  'RACE_NAME',  'ETHNICITY_NAME',\n",
    "       'LENGTH_OF_STAY',  'PAT_AGE_GROUP', \n",
    "       'RISK_MORTALITY', 'ILLNESS_SEVERITY']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at getting data by rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting by rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rows, we use: \n",
    "\n",
    "- `.loc` - **loc**ates by name\n",
    "\n",
    "We will use 0, 1, 2,....  To indicate the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a single column element by using the row number and the column name\n",
    "\n",
    "Again, we use `.loc` but this time we indicate the row and column:\n",
    "\n",
    "for row 0 and column 'PUBLIC_HEALTH_REGION' we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'ETHNICITY_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Subsetting by Conditional selections\n",
    "We’ve gone over how to select columns and rows, but what if we want to make a conditional selection? \n",
    "\n",
    "In your data set we will need to subset the data depending on how you use the data.  First we will need to know what are the data numbers for each County.  So we will subset the data by year '2018' to match the year I collected my County Health Rankings data.  \n",
    "\n",
    "Remember we created the YEAR column by splitting the discharge string. Therefore, YEAR is a string.\n",
    "\n",
    "Also, text within a column is case sensitive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf = df[df['YEAR'] == '2018']\n",
    "\n",
    "yeardf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the row numbers (index) do not start with 0 and a lot of numbers are skipped.** To fix this we reindex the subset dataframe.  This does not effect the original dataframe `df` because we created a new dataframe `yeardf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf = yeardf.reset_index(drop=True)\n",
    "yeardf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf.to_csv('myData2018.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf['PAT_COUNTY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Appropriate Data from County Health Rankings\n",
    "\n",
    "If you get a file not found error, one of two things have ocurred:\n",
    "1. You mispelled the filename and it is case sensitive, or\n",
    "2. The file is not in the same directory as this notebook file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHRdf = pd.read_csv('CCDEMO.csv')\n",
    "CHRdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the `FIPS` code column to  the `PAT_CTY_CODE` column in the discharge data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = yeardf[yeardf['PAT_COUNTY']=='CAMERON']\n",
    "subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subCHRdf=CHRdf[CHRdf['County']=='Cameron']\n",
    "subCHRdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Discharge data `df` the county code for Cameron is 61.  In the CHR data `CHRdf` theFIPS code is 48061.  If we compare other counties you will see that we need to add 48000 to the `PAT_CTY_CODE` in the Discharge data so that wee wioll have a matching key in each file.\n",
    "\n",
    "**Only run the next cell once.  If you run it multiple times, you will add 48000 to the code each time!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf['PAT_CTY_CODE'] = yeardf['PAT_CTY_CODE'] + 48000\n",
    "yeardf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Save and then Read our file, so we can start at this cell if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeardf.to_csv('myData2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('myData2018.csv', dtype={'RECORD_ID': 'category', 'YEAR': 'category', 'PAT_COUNTY': 'category',\n",
    "                                      'SEX_CODE': 'category',  'ETHNICITY_NAME': 'category'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the discharge file from long format to wide format\n",
    "\n",
    "Long format - more than one row for each county\n",
    "Wide format - only one row per county.\n",
    "\n",
    "To do this we will need to determin how to tranform data. For Example:\n",
    "1. Going from race as White Black and Other to columns Race-White, Race Black, Race Other indicating the number of of each per county.\n",
    "2. Same thing for Ethnicity\n",
    "3. Taking Length of Stay and converting it to Average Length of Stay\n",
    "4. All of these may be stratified by sex or Age group\n",
    "\n",
    "### We need to determine what columns are needed for our study.\n",
    "\n",
    "For my study on Colon Cancer I am looking at the number of hospitalization in adults, Hispanic or Non-hispanic ethnicity, and I will go ahead stratify by sex. \n",
    "\n",
    "I will need these transformations:\n",
    "1. Male\n",
    "    - Greater than 18 age group\n",
    "    - Total Hispanics\n",
    "    - Total Non-hispanic\n",
    "2. Female\n",
    "    - Greater than 18 age group\n",
    "    - Total Hispanics\n",
    "    - Total Non-hispanic\n",
    "3. All\n",
    "    - Greater than 18 age group\n",
    "    - Total Hispanics\n",
    "    - Total Non-hispanic\n",
    "    \n",
    "To do this we are going to create two new dataframe aggregations. In the first we will do transforms 1 & 2 above  and only need columns 'RECORD_ID', 'PAT_CTY_CODE', 'SEX_CODE','ETHNICITY_NAME'\n",
    "\n",
    "In the last transformation we sum up the Hispanic and non-hispanic Columns\n",
    "\n",
    "In these transformations well be grouping and counting records within the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agDF1 = df[['RECORD_ID', 'PAT_CTY_CODE', 'SEX_CODE',\n",
    "       'ETHNICITY_NAME']]\n",
    "\n",
    "agDF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = agDF1.groupby(['PAT_CTY_CODE','SEX_CODE','ETHNICITY_NAME']).agg([ 'count']).unstack().unstack()\n",
    "newDF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above (newDF), the hospitalizations are organized by the number of Hispanic Females, Hispanic Males, Non-hispanic Females , and non-Hispanic Males by PAT_CTY_CODE \n",
    "\n",
    "Lets rename the columns Hispanic_Females, Hispanic_Males, Non-hispanic_Females , and Non-hispanic_Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.columns = newDF.columns.get_level_values(0)\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.columns = ['Hispanic_Females', 'Hispanic_Males', 'Non-hispanic_Females' , 'Non-hispanic_Males']\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF= newDF.reset_index()\n",
    "newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now add Total_Hispanics\n",
    "\n",
    "total the hispanics by row.\n",
    "\n",
    "Then repeat for Non-hispanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.insert(loc=3, column='Total_Hispanics',value ='')\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF['Total_Hispanics'] = newDF['Hispanic_Females'] + newDF['Hispanic_Males']\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.insert(loc=6, column='Total_Non-hispanics',value ='')\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF['Total_Non-hispanics'] = newDF['Non-hispanic_Females'] + newDF['Non-hispanic_Males']\n",
    "newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Save and then Read our file, so we can start at this cell if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv('myData2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('myData2018.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will join our data from `CHRdf` and `df`.  \n",
    "\n",
    "First we will rename the `PAT_CTY_CODE` column in `df` to `FIPS` to match the column name in `CHRdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'PAT_CTY_CODE': 'FIPS'})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the `CHRdf` from County Health Rankings, because it has all 254 counties - so it will be our left dataset.\n",
    "the `df` ddischarge data will be our right dataset.  We will be doing a left join.  It takes all row indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pd.merge(CHRdf,df, on='FIPS', how='left')\n",
    "mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize data\n",
    "\n",
    "To use the sex stratified Ethnicities, I would also need the sex stratified population of the county from County Health Rankings.  I have included these hear just to show you how to aggregate the discharge data.  For my study I will only use the total Ethnicities.\n",
    "\n",
    "Calculate:\n",
    "1. %Uninisured\n",
    "2. %Total_Hispanics hospitalized\n",
    "3. %Total_Non-hispanics hospitalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.insert(loc=4, column='%Uninsured',value ='')\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['%Uninsured'] = (mdf['Uninsured_Adults']/mdf['Population'])*100\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.insert(loc=10, column='%Total_Hispanics',value ='')\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['%Total_Hispanics'] = (mdf['Total_Hispanics'] /mdf['Population'])*100\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.insert(loc=14, column='%Total_Non-hispanics',value ='')\n",
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf['%Total_Non-hispanics'] = (mdf['Total_Non-hispanics'] /mdf['Population'])*100\n",
    "mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mdf.to_csv('mapData2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
